{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb304d0d",
   "metadata": {},
   "source": [
    "### Installations and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9a60656",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.59.3)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b093008f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.59.3)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ba86b68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Obtaining dependency information for opencv-python from https://files.pythonhosted.org/packages/38/d2/3e8c13ffc37ca5ebc6f382b242b44acb43eb489042e1728407ac3904e72f/opencv_python-4.8.1.78-cp37-abi3-win_amd64.whl.metadata\n",
      "  Downloading opencv_python-4.8.1.78-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\kkeer\\anaconda3\\lib\\site-packages (from opencv-python) (1.24.3)\n",
      "Downloading opencv_python-4.8.1.78-cp37-abi3-win_amd64.whl (38.1 MB)\n",
      "   ---------------------------------------- 0.0/38.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/38.1 MB 2.0 MB/s eta 0:00:20\n",
      "   ---------------------------------------- 0.2/38.1 MB 2.1 MB/s eta 0:00:19\n",
      "   ---------------------------------------- 0.3/38.1 MB 2.4 MB/s eta 0:00:16\n",
      "   ---------------------------------------- 0.4/38.1 MB 2.4 MB/s eta 0:00:16\n",
      "    --------------------------------------- 0.6/38.1 MB 2.6 MB/s eta 0:00:15\n",
      "    --------------------------------------- 0.7/38.1 MB 2.7 MB/s eta 0:00:14\n",
      "    --------------------------------------- 0.9/38.1 MB 2.8 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 1.1/38.1 MB 2.9 MB/s eta 0:00:13\n",
      "   - -------------------------------------- 1.2/38.1 MB 3.0 MB/s eta 0:00:13\n",
      "   - -------------------------------------- 1.4/38.1 MB 3.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.6/38.1 MB 3.2 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.8/38.1 MB 3.3 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 2.0/38.1 MB 3.4 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 2.2/38.1 MB 3.5 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 2.5/38.1 MB 3.6 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 2.7/38.1 MB 3.7 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 2.9/38.1 MB 3.7 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 3.2/38.1 MB 3.8 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 3.4/38.1 MB 3.9 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 3.7/38.1 MB 4.0 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 4.0/38.1 MB 4.1 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 4.3/38.1 MB 4.2 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 4.6/38.1 MB 4.3 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 4.8/38.1 MB 4.3 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 5.1/38.1 MB 4.4 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 5.4/38.1 MB 4.5 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 5.7/38.1 MB 4.6 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 6.0/38.1 MB 4.6 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 6.3/38.1 MB 4.7 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 6.7/38.1 MB 4.8 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 7.0/38.1 MB 4.9 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 7.4/38.1 MB 5.0 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 7.7/38.1 MB 5.0 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 7.9/38.1 MB 5.0 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 8.2/38.1 MB 5.1 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 8.5/38.1 MB 5.1 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 8.9/38.1 MB 5.2 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 9.3/38.1 MB 5.3 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 9.7/38.1 MB 5.4 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 10.1/38.1 MB 5.4 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 10.6/38.1 MB 5.8 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 11.0/38.1 MB 6.1 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 11.4/38.1 MB 6.3 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 11.8/38.1 MB 6.5 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 12.2/38.1 MB 6.8 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 12.7/38.1 MB 7.0 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 13.1/38.1 MB 7.3 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 13.5/38.1 MB 7.4 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 13.9/38.1 MB 7.6 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 14.4/38.1 MB 7.7 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 14.9/38.1 MB 7.8 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 15.3/38.1 MB 8.1 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 15.8/38.1 MB 8.2 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 16.2/38.1 MB 8.4 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 16.6/38.1 MB 8.5 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 17.1/38.1 MB 8.6 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 17.7/38.1 MB 8.8 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 18.2/38.1 MB 9.1 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 18.6/38.1 MB 9.5 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 19.1/38.1 MB 9.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 19.6/38.1 MB 9.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 20.0/38.1 MB 9.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 20.6/38.1 MB 9.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 21.1/38.1 MB 9.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 21.6/38.1 MB 9.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 22.1/38.1 MB 10.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 22.6/38.1 MB 10.1 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 23.1/38.1 MB 10.1 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 23.7/38.1 MB 10.2 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 24.1/38.1 MB 10.2 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 24.6/38.1 MB 10.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 25.2/38.1 MB 10.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 25.7/38.1 MB 10.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 26.3/38.1 MB 10.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 26.8/38.1 MB 10.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 27.5/38.1 MB 10.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 28.1/38.1 MB 11.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 28.8/38.1 MB 11.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 29.3/38.1 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 29.8/38.1 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 30.4/38.1 MB 11.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 30.9/38.1 MB 11.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 31.2/38.1 MB 11.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 31.6/38.1 MB 11.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 31.9/38.1 MB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 32.5/38.1 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 33.1/38.1 MB 11.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 33.5/38.1 MB 11.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 34.1/38.1 MB 11.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 34.7/38.1 MB 11.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 35.4/38.1 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 36.0/38.1 MB 11.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 36.6/38.1 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  37.2/38.1 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  37.9/38.1 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.1/38.1 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.1/38.1 MB 10.7 MB/s eta 0:00:00\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.8.1.78\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d5c28a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 as cv\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, MaxPool2D, BatchNormalization, Dropout\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten\n",
    "\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.optimizers.legacy import Adam\n",
    "from keras.optimizers import schedules\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.image import imread\n",
    "from PIL import Image\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#imports for mapping section of code \n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32646d20",
   "metadata": {},
   "source": [
    "### Loading the Image Localization Model and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29f88cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USED THE CLASS FROM https://towardsdatascience.com/model-sub-classing-and-custom-training-loop-from-scratch-in-tensorflow-2-cc1d4f10fb4e\n",
    "class SmokeDetection(tf.keras.models.Model):\n",
    "    def __init__(self, givenModel, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Define Model\n",
    "        self.model = givenModel\n",
    "\n",
    "    def call(self, input, **kwargs):\n",
    "        return self.model(input, **kwargs)\n",
    "\n",
    "    def compile(self, optimizer, class_loss, local_loss, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "\n",
    "        self.local_loss = local_loss\n",
    "        self.class_loss = class_loss\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def train_step(self, batch, **kwargs):\n",
    "        # imgs = batch[0]\n",
    "        # gt_class_with_bbox = batch[1]\n",
    "        # batch[1][0] = TRUE class and batch[1][1] is array of TRUE coordinates\n",
    "\n",
    "        imgs, gt_class_with_bbox = batch\n",
    "\n",
    "        # imgs, gt_class_with_bbox = self.imageAug(old_imgs, old_gt_class_with_bbox)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "          # class_pred = self.model(imgs, training=True)[0]\n",
    "          # bbox_pred = self.model(imgs, training=True)[1]\n",
    "\n",
    "            class_pred, bbox_pred = self.model(imgs, training=True)\n",
    "\n",
    "            class_loss = self.class_loss(gt_class_with_bbox[0], class_pred)\n",
    "\n",
    "            gt_bbox = tf.cast(gt_class_with_bbox[1], tf.float32)\n",
    "\n",
    "            local_loss = self.local_loss(gt_bbox, bbox_pred)\n",
    "\n",
    "            # We want loss to worry more about localizing the smoke rather than classifying it\n",
    "            # This follows from the fact that our images only belong to 1 class and the main problem\n",
    "            # is detecting where exactly in the picture the smoke is\n",
    "            tot_loss = class_loss * 0.5 + local_loss\n",
    "            change = tape.gradient(tot_loss, self.model.trainable_variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(change, self.model.trainable_variables))\n",
    "\n",
    "        return {\"total_loss\": tot_loss, \"class_loss\": class_loss, \"local_loss\": local_loss}\n",
    "\n",
    "    # THESE NEXT TWO FUNCTIONS ARE OURS\n",
    "    def imageAug(self, imgs, cl_with_bbox):\n",
    "\n",
    "        new_imgs = []\n",
    "        new_cl_with_bbox = []\n",
    "\n",
    "        for indx in range(2):\n",
    "            temp_bbox = [BoundingBox(x1=cl_with_bbox[indx][0], y1=cl_with_bbox[indx][1], x2=cl_with_bbox[indx][2], y2=cl_with_bbox[indx][3], label=cl_with_bbox[indx][0])]\n",
    "            temp_img = [imgs]\n",
    "\n",
    "            bb = BoundingBoxesOnImage(temp_bbox, shape=(244, 244))\n",
    "\n",
    "            aug_img, aug_bb = data_augment(images=temp_img, bounding_boxes=bb)\n",
    "            new_bbox = [[0], [aug_bb[0].x1, aug_bb[0].y1, aug_bb[0].x2, aug_bb[0].y2]]\n",
    "\n",
    "            new_imgs.append(aug_img)\n",
    "            cl_with_bbox.append(new_bbox)\n",
    "\n",
    "        return new_imgs, cl_with_bbox\n",
    "    \n",
    "    # Added test_step that basically also calculates the loss function of the Validation Data\n",
    "    # Validation data is how we can interpret when the model has finished learning or how well it is really doing\n",
    "    def test_step(self, validation_data):\n",
    "        valid_data = validation_data\n",
    "\n",
    "        val_imgs, val_classes_with_bbox = valid_data\n",
    "\n",
    "        val_class_pred, val_bbox_pred = self.model(val_imgs, training=False)\n",
    "        val_class_loss = self.class_loss(val_classes_with_bbox[0], val_class_pred)\n",
    "\n",
    "        val_gt_bbox = tf.cast(val_classes_with_bbox[1], tf.float32)\n",
    "        val_local_loss = self.local_loss(val_gt_bbox, val_bbox_pred)\n",
    "\n",
    "        val_tot_loss = val_class_loss * 0.5 + val_local_loss\n",
    "\n",
    "        return {\"total_loss\": val_tot_loss, \"class_loss\": val_class_loss, \"local_loss\": val_local_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ec24806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USED basic outline of model code from website above but MODIFIED the model architecture through lots of experimentation with models\n",
    "def create_model():\n",
    "    # Define an input layer that takes in size (244, 244, 3) where 3 is number of channels - RGB\n",
    "    input = Input(shape=(244, 244, 3))\n",
    "    \n",
    "    # We use the pre-trained model VGG16 with its weights\n",
    "    base_model = VGG16(weights = 'imagenet', include_top = False, input_shape = (244, 244, 3))\n",
    "    \n",
    "    # MODIFIED: We froze every layer but the last convolutional block to keep the features that the model has learned\n",
    "    for layer in base_model.layers[:-5]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # MODIFIED ARCHITECTURE\n",
    "    \n",
    "    # This is the model for classification. We just have one fully connected Dense layer with 256 units\n",
    "    # Dropout is added to prevent the model from fullying relying on a select few number of neurons in the layer\n",
    "    class_out = GlobalAveragePooling2D()(base_model.output)\n",
    "    class_out = Dense(units=256)(class_out)\n",
    "    class_out = Dropout(0.2)(class_out)\n",
    "    \n",
    "    # Sigmoid activation function in the end because this is a binary value either 0 or 1\n",
    "    class_out = Dense(units=1, activation='sigmoid')(class_out)\n",
    "    \n",
    "    # This is the model for localization. We have 3 fully connected Dense layers with different number of units\n",
    "    local_out = GlobalAveragePooling2D()(base_model.output)\n",
    "    local_out = Dense(units=2048)(local_out)\n",
    "    local_out = Dense(units=1024)(local_out)\n",
    "    local_out = Dropout(0.2)(local_out)\n",
    "    local_out = Dense(units=512)(local_out)\n",
    "    local_out = Dropout(0.3)(local_out)\n",
    "    \n",
    "    # Relu activation function between we did not normalize bounding box values so they can be larger than 1 \n",
    "    local_out = Dense(units=4, activation='relu')(local_out)\n",
    "    \n",
    "    # Combine models into one model \n",
    "    model = Model(inputs = base_model.input, outputs = [class_out, local_out])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e0309b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to resize the images to fit the pre-trained model we are going to use\n",
    "def resizeImages(imgs, new_width, new_height):\n",
    "    \n",
    "    resizedImages = []\n",
    "    \n",
    "    # Just resizes\n",
    "    for indx in range(len(imgs)):\n",
    "        resizedImages.append(imgs[indx].resize((new_width, new_height)))\n",
    "    \n",
    "    return resizedImages\n",
    "\n",
    "# We normalize the pixel values because it is more computationally efficient to deal with numbers between 0-1 rather than 0-255\n",
    "def normalize(imgs):\n",
    "    \n",
    "    normalized = []\n",
    "    \n",
    "    # Loop through each img, convert to array of pixel values, and divide each by 255\n",
    "    for img in imgs:\n",
    "        img_arr = np.array(img)\n",
    "        img_arr = img_arr / 255\n",
    "        normalized.append(img_arr)\n",
    "        \n",
    "    return normalized\n",
    "\n",
    "# If we scale the images, we have to scale the bounding boxes now as well otherwise it wont localize the smoke anymore\n",
    "def scaleBoundingBoxes(bboxes, new_width, new_height, prev_width, prev_height):\n",
    "    \n",
    "    scaled_bboxes = []\n",
    "    \n",
    "    for indx in range(len(all_files)):\n",
    "        \n",
    "        # Get the scaling factors\n",
    "        x_Scale_Factor = new_width / prev_width\n",
    "        y_Scale_Factor = new_height / prev_height\n",
    "        \n",
    "        # Get the new bottom left corner and top right corner \n",
    "        xmin = bboxes[0] * x_Scale_Factor\n",
    "        ymin = bboxes[1] * y_Scale_Factor\n",
    "        xmax = bboxes[2] * x_Scale_Factor\n",
    "        ymax = bboxes[3] * y_Scale_Factor\n",
    "        \n",
    "        scaled_bboxes.append([xmin, ymin, xmax, ymax])\n",
    "    \n",
    "    return scaled_bboxes\n",
    "\n",
    "# In order to understand image localization, we need a function to draw the bounding boxes on the image\n",
    "# We have a default value for predicted bounding box because sometimes we just want to draw the ground truth bounding box only\n",
    "def drawBoundingBoxes(img, gt_bbox, pred_bbox = []):\n",
    "\n",
    "    gt_boxWidth = gt_bbox[2] - gt_bbox[0]\n",
    "    gt_boxHeight = gt_bbox[3] - gt_bbox[1]\n",
    "    \n",
    "    # We might not always have a predicted box if we havent ran predictions yet. So we need to check for that case\n",
    "    if len(pred_bbox) != 0:\n",
    "        pred_boxWidth = pred_bbox[2] - pred_bbox[0]\n",
    "        pred_boxHeight = pred_bbox[3] - pred_bbox[1]\n",
    "    \n",
    "    # Show the image\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    # Plot the rectangle or bounding box that localizes the smoke\n",
    "    gt_rectangle = Rectangle((gt_bbox[0], gt_bbox[1]), gt_boxWidth, gt_boxHeight, facecolor='none', edgecolor='black')\n",
    "    plt.gca().add_patch(gt_rectangle)\n",
    "    \n",
    "    # If we have a predicted box then we also display what the prediction looks like as well\n",
    "    if len(pred_bbox) != 0:\n",
    "        pred_rectangle = Rectangle((pred_bbox[0], pred_bbox[1]), pred_boxWidth, pred_boxHeight, facecolor='none', edgecolor='red')\n",
    "        plt.gca().add_patch(pred_rectangle)\n",
    "    \n",
    "\n",
    "def predict(model, test_img):\n",
    "    pred_img = np.expand_dims(test_img, 0)\n",
    "    pred = model.predict(pred_img)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "81fd6854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function that we wrote however the math and idea of IoU is not ours\n",
    "def IoU(model, test_imgs, test_bboxes):\n",
    "    \"\"\"\n",
    "    This function will find the mean of the IoU of the specified model on the test images\n",
    "    \"\"\"\n",
    "    iou = 0\n",
    "    \n",
    "    for indx in range(len(test_imgs)):\n",
    "        \n",
    "        resized = resizeImages([test_imgs[indx]], 244, 244)\n",
    "        normalized = normalize(resized)\n",
    "        \n",
    "        pred = predict(model, normalized[0])\n",
    "        \n",
    "        pred = scaleBoundingBoxes(pred[1][0], 640, 480, 244, 244)[0]\n",
    "        \n",
    "        # Calculate the area of the ground truth box\n",
    "        gt_width = test_bboxes[indx][2] - test_bboxes[indx][0]\n",
    "        gt_height = test_bboxes[indx][3] - test_bboxes[indx][1]\n",
    "        \n",
    "        gt_area = gt_width * gt_height\n",
    "        \n",
    "        # Calculate the area of the prediction box\n",
    "        pred_width = pred[2] - pred[0]\n",
    "        pred_height = pred[3] - pred[1]\n",
    "        \n",
    "        pred_area = pred_width * pred_height\n",
    "        \n",
    "        # Calculates intersection of the boxes\n",
    "        x_intersect_min = max(test_bboxes[indx][0], pred[0])\n",
    "        y_intersect_min = max(test_bboxes[indx][1], pred[1])\n",
    "        x_intersect_max = min(test_bboxes[indx][2], pred[2])\n",
    "        y_intersect_max = min(test_bboxes[indx][3], pred[3])\n",
    "        \n",
    "        # IoU cant be negative so we need to make sure area is not lower than 0\n",
    "        intersect_width = max(0, x_intersect_max - x_intersect_min)\n",
    "        intersect_height = max(0, y_intersect_max - y_intersect_min)\n",
    "        \n",
    "        intersection = intersect_width * intersect_height\n",
    "        \n",
    "        # Calculate the union area\n",
    "        union = gt_area + pred_area - intersection\n",
    "        \n",
    "        iou += (intersection / union)\n",
    "    \n",
    "    iou /= len(test_imgs)\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b1ec296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kkeer\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kkeer\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model = SmokeDetection(create_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a07feef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kkeer\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\legacy\\save.py:538: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x1c3b4721cd0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.load_weights('Models/smokeModelWithDropout')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3cc96e",
   "metadata": {},
   "source": [
    "### Pre-load Images and Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "225b37e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('SmokeDataset.csv')\n",
    "img_dir = 'day_time_wildfire_v2/images/'\n",
    "all_files = df.to_numpy()\n",
    "\n",
    "# Function to get all of the images in the directory\n",
    "def getImagesFromDirectory(img_dir, all_files):\n",
    "    \n",
    "    imgs = []\n",
    "    \n",
    "    # Loops through all of the images in the directory, opens them, and adds them to the list\n",
    "    for indx in range(len(all_files)):\n",
    "        tempImage = Image.open(os.path.join(img_dir, all_files[indx, 0]) + '.jpeg')\n",
    "        \n",
    "        imgs.append(tempImage)\n",
    "        \n",
    "    return imgs\n",
    "\n",
    "# Gets all of the classes and bounding boxes for each image\n",
    "def getLabelsFromDirectory(all_files):\n",
    "    \n",
    "    classes = []\n",
    "    bboxes = []\n",
    "    \n",
    "    # Reads the csv file that we made and gets the bounding boxes and class of each image (they should all be smoke labeled)\n",
    "    for indx in range(len(all_files)):\n",
    "         \n",
    "        # Getting bottom left corner and top right corner to define the bounding box\n",
    "        xmin = all_files[indx, 4]\n",
    "        ymin = all_files[indx, 5]\n",
    "        xmax = all_files[indx, 6]\n",
    "        ymax = all_files[indx, 7]\n",
    "        \n",
    "        bboxes.append([xmin, ymin, xmax, ymax])\n",
    "        \n",
    "        # If it is smoke labeled we put a 0 and else we put a 1\n",
    "        class_name = all_files[indx, 3]\n",
    "        if class_name == 'smoke':\n",
    "            classes.append(0)\n",
    "        else:\n",
    "            classes.append(1)\n",
    "        \n",
    "    return classes, bboxes\n",
    "\n",
    "images = getImagesFromDirectory(img_dir, all_files)\n",
    "classes, bboxes = getLabelsFromDirectory(all_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78730a50",
   "metadata": {},
   "source": [
    "### Loading Forest Fire Area Predictor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "825760f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsRegressor(n_neighbors=20)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsRegressor</label><div class=\"sk-toggleable__content\"><pre>KNeighborsRegressor(n_neighbors=20)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsRegressor(n_neighbors=20)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fires = pd.read_csv('wildfiredb_2016_subset.csv')\n",
    "\n",
    "columns = ['ELEV_mean', 'SLP_mean', 'EVT_mean', 'EVH_mean', 'TEMP_min', 'TEMP_max', 'PRCP', 'WSPD_ave', 'PRES_ave']\n",
    "X = fires[columns]  # Independent variables\n",
    "\n",
    "# Apply a logarithm transform to make the data more normalized (initially skewed with many values close to 0)\n",
    "fires['frp_scaled'] = np.log10(fires['frp'] + 1)\n",
    "Y = fires['frp_scaled']  # Dependent variable\n",
    "\n",
    "# Separate 80% data into training set and 20% into test set\n",
    "# random_state=0 ensures that the results are reproducible\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.20, random_state=0)\n",
    "\n",
    "# Applying k-nearest neighbors model\n",
    "RF_model = RandomForestRegressor(n_estimators = 50, max_depth = 30, max_samples=5000)\n",
    "RF_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e7a867",
   "metadata": {},
   "source": [
    "### Load California Fire Incidents Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d10fce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just loads data that is in the California Fire jupyter notebook and gets ready to use it\n",
    "df = pd.read_csv(r\"California_Fire_Incidents.csv\") \n",
    "df = df.drop_duplicates()\n",
    "df = df.drop(columns = ['CanonicalUrl','Location','UniqueId','ConditionStatement','ControlStatement', 'SearchDescription','SearchKeywords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e81264",
   "metadata": {},
   "source": [
    "### Menu Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a4568f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayBasicMenu():\n",
    "    print(\"Welcome to the Forest Fire Visualizer!\")\n",
    "    \n",
    "    print(\"\\nMenu\")\n",
    "    print(\"0. Quit\")\n",
    "    print(\"1. Get Statistics about a specific California County\")\n",
    "    print(\"2. View Fire Radiative Power Predictor Model\")\n",
    "    print(\"3. View Smoke Detection Model \")\n",
    "    \n",
    "    user_input = int(input(\"\\nPlease enter the number beside the menu selection that you would like to view: \"))\n",
    "    \n",
    "    return user_input\n",
    "\n",
    "def displayAreaPredictorMenu():\n",
    "    print(\"Welcome to our Forest Fire Radiative Power Predictor!\")\n",
    "    \n",
    "    print(\"\\nOptions\")\n",
    "    print(\"0. Quit\")\n",
    "    print(\"1. Get Information on the Parameters that our Model takes in\")\n",
    "    print(\"2. View a Correlation Heatmap of the Parameters that our Model takes in\")\n",
    "    print(\"3. Predict Fire Radiative Power\")\n",
    "    \n",
    "    user_input = int(input(\"\\nPlease enter the number beside the menu selection that you would like to view: \"))\n",
    "    \n",
    "    return user_input\n",
    "\n",
    "def displaySmokeDetectionMenu():\n",
    "    print(\"Welcome to the our Smoke Detection Model!\")\n",
    "    \n",
    "    print(\"\\nOptions\")\n",
    "    print(\"0. Quit\")\n",
    "    print(\"1. Get Information on how the Model Works\")\n",
    "    print(\"2. Show Predictions on a Specific Image in the Dataset (Requires Filename)\")\n",
    "    print(\"3. Show Predictions on a 9 Random Images in the Dataset\")\n",
    "    print(\"4. Get the IoU Accuracy on a Specific Range of Images in the Dataset\")\n",
    "    print(\"5. Simulate Real-Time Detection by Applying the Model to a Video\")\n",
    "    \n",
    "    user_input = int(input(\"\\nPlease enter the number beside the menu selection that you would like to view: \"))\n",
    "    \n",
    "    return user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "322b309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics_county(county_name):\n",
    "    print(\"\")\n",
    "    county_data = df[df['Counties'] == county_name]\n",
    "\n",
    "    if county_data.empty:\n",
    "        return f\"No data found for {county_name}.\"\n",
    "\n",
    "    total_incidents = county_data.shape[0]\n",
    "    total_acres_burned = county_data['AcresBurned'].sum()\n",
    "    average_acres_burned = county_data['AcresBurned'].mean()\n",
    "\n",
    "    return f\"Statistics for {county_name}:\\nTotal Incidents: {total_incidents}\\nTotal Acres Burned: {total_acres_burned}\\nAverage Acres Burned per Incident: {average_acres_burned:.2f}\"\n",
    "\n",
    "def frpPredictorInformation():\n",
    "    print(\"\\n\\nThe following information is from the WildfireDB Page.\")\n",
    "    print(\"Our model takes in the following parameters from WildfireDB. Each value refers to one 375 by 375 meter squared area in the U.S.\\n\")\n",
    "    \n",
    "    # Information from https://wildfire-modeling.github.io/\n",
    "    print('ELEV_mean: Mean elevation over the observed area from 2016.')\n",
    "    print('SLP_mean: Mean slope over the observed area from 2016')\n",
    "    print('EVT_mean: Mean existing vegetation type.')\n",
    "    print('EVH_mean: Mean existing vegetation height.')\n",
    "    print('TEMP_min: Minimum temperature.')\n",
    "    print('TEMP_max: Maximum temoerature.')\n",
    "    print('PRCP: Precipitation.')\n",
    "    print('WSPD_ave: Average relative wind speed.')\n",
    "    print('PRES_ave: Average atmospheric pressure.')\n",
    "\n",
    "    print('\\nFire Radiative Power: The model predicts the fire radiative power in Watts given the above vegetation and weather conditions.\\n\\n')\n",
    "\n",
    "def viewHeatMap():\n",
    "    heatmap = sns.heatmap(fires[cols].corr()[['frp']].sort_values(by='frp', ascending=False), vmin=-1, vmax=1, annot=True)\n",
    "    heatmap.set_title('Correlations Between Parameters and Fire Radiative Power (FRP)');\n",
    "\n",
    "def predictFRP():\n",
    "    print(\"Please enter the following values as floating point values (decimals): \\n\")\n",
    "    \n",
    "    # Takes in Weather and Vegetation Data\n",
    "    elev_mean = float(input(\"Mean Elevation: \"))\n",
    "    slp_mean = float(input(\"Mean Slope: \"))\n",
    "    evt_mean = float(input(\"Mean Vegetation Type: \"))\n",
    "    evh_mean = float(input(\"Mean Vegetation Height: \"))\n",
    "    temp_min = float(input(\"Minimum Temperature: \"))\n",
    "    temp_max = float(input(\"Maximum Temperature: \"))\n",
    "    prcp = float(input(\"Precipitation: \"))\n",
    "    wspd_ave = float(input(\"Average relative wind speed: \"))\n",
    "    pres_ave = float(input(\"Average atmospheric pressure: \"))\n",
    "    \n",
    "    frp_predict = RF_model.predict([[elev_mean, slp_mean, evt_mean, evh_mean, temp_min, temp_max, prcp, wspd_ave, pres_ave]])\n",
    "    \n",
    "    # Print the predicted FRP after reversing the log transform that was applied\n",
    "    print(f\"Predicted Fire Radiative Power: {10**(frp_predict[0]-1)} Watts.\")\n",
    "\n",
    "def smokeDetectionInformation():\n",
    "    print(\"\\n\\nOur custom smoke detection model is an image localization model that predicts where the smoke in an image is.\")\n",
    "    \n",
    "    print(\"Our model utilizes transfer learning by utilizing VGG16 as the pre-trained base model. We chose VGG16 because\")\n",
    "    print(\"it is lightweight compared to many other pre-trained models. It only has 16 layers with weights which reduces the large\")\n",
    "    print(\"number of parameters that other models have. Additionally, VGG16 has been trained on over 1000 different classes and therefore\")\n",
    "    print(\"has learned many high level features that we wanted to use for our purpose. On top of the base model we added our own custom\")\n",
    "    print(\"architecture for classification (determining if smoke is in the image or not by giving a probability) and localization\")\n",
    "    print(\"(coordinates of the bounding box that highlights the smoke in the image). The model was trained on 1533 images with 438\")\n",
    "    print(\"images for cross-validaton and tested on 219 images.\\n\\n\")\n",
    "    \n",
    "    \n",
    "def predictOnFile():\n",
    "    file = input(\"Please enter JUST the file name (no path) the image in the dataset: \")\n",
    "    \n",
    "    print(\"Black represents the labeled bounding box\")\n",
    "    print(\"Red represents the predicted bounding box\")\n",
    "    \n",
    "    for indx in range(len(all_files)):\n",
    "        if all_files[indx, 0] == file:\n",
    "            # Image manipulation so it fits model input\n",
    "            resized = resizeImages([images[indx]], 244, 244)\n",
    "            normalized = normalize(resized)\n",
    "            \n",
    "            # Predict\n",
    "            pred = predict(best_model, normalized[0])\n",
    "            \n",
    "            # Because our images are actually 640 x 480 we want to scale the bounding box prediction and image back to normal\n",
    "            scaled_pred = scaleBoundingBoxes(pred[1][0], 640, 480, 244, 244)\n",
    "            \n",
    "            # Draw the Box\n",
    "            drawBoundingBoxes(images[indx], bboxes[indx], scaled_pred[0])\n",
    "            \n",
    "            return\n",
    "    \n",
    "    print(\"Could not find file\")\n",
    "\n",
    "def predictOnRandom():\n",
    "    # Let's try to visualize multiple different predictions using the NoDropout Model\n",
    "    plt.figure(figsize=(18, 15))\n",
    "\n",
    "    for subplot in range(9):\n",
    "        plt.subplot(3, 3, subplot + 1)\n",
    "\n",
    "        # Get a random index from the test images\n",
    "        image_indx = np.random.randint(0, len(images))\n",
    "        \n",
    "        resized = resizeImages([images[image_indx]], 244, 244)\n",
    "        normalized = normalize(resized)\n",
    "        \n",
    "        # predict on the test image\n",
    "        pred = predict(best_model, normalized[0])\n",
    "        \n",
    "        scaled_pred = scaleBoundingBoxes(pred[1][0], 640, 480, 244, 244)\n",
    "        \n",
    "        # pred[0] has the classification probability while pred[1][0] is the array of bounding box\n",
    "        drawBoundingBoxes(images[image_indx], bboxes[image_indx], scaled_pred[0])\n",
    "\n",
    "def getIoUAccuracy():\n",
    "    \n",
    "    beg_input = int(input(\"Enter the beginning endpoint for the range of images you want to test on (0-2189)\"))\n",
    "    end_input = int(input(\"Enter the ending endpoint for the range of images you want to test on (0-2189)\\n\"))\n",
    "    \n",
    "    while(beg_input >= end_input):\n",
    "        print(\"First number cannot be bigger than Second!!!\")\n",
    "        beg_input = int(input(\"Enter the beginning endpoint for the range of images you want to test on (0-2189)\"))\n",
    "        end_input = int(input(\"Enter the ending endpoint for the range of images you want to test on (0-2189)\\n\"))\n",
    "    \n",
    "    while(not(0 <= beg_input <= 2189) or not(0 <= end_input <= 2189)):\n",
    "        print(\"Numbers must be IN RANGE!!!\")\n",
    "        beg_input = int(input(\"Enter the beginning endpoint for the range of images you want to test on (0-2189)\"))\n",
    "        end_input = int(input(\"Enter the ending endpoint for the range of images you want to test on (0-2189)\\n\"))\n",
    "    \n",
    "    print(IoU(best_model, images[beg_input:end_input], bboxes[beg_input:end_input]))\n",
    "\n",
    "# https://youtube.com/watch?v=AxIc-vGaHQ0 Used this video to learn \n",
    "def applyToVideo():\n",
    "    cap = cv.VideoCapture('InputVideo/20160604-FIRE-smer-tcs3-mobo-c.mp4')\n",
    "    width = int(cap.get(cv.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # What we are writing new video to\n",
    "    output = cv.VideoWriter('OutputVideo/Output_20160604-FIRE-smer-tcs3-mobo-c.mp4', cv.VideoWriter_fourcc(*'MP4V'), 20.0, (width, height))\n",
    "    \n",
    "    xScaleFactor = width / 244\n",
    "    yScaleFactor = height / 244\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        img = frame\n",
    "        img = cv.resize(img, (244, 244))\n",
    "        img = img / 255\n",
    "        \n",
    "        pred = predict(best_model, img)\n",
    "        \n",
    "        xmin = int(pred[1][0][0] * xScaleFactor)\n",
    "        ymin = int(pred[1][0][1] * yScaleFactor)\n",
    "        xmax = int(pred[1][0][2] * xScaleFactor)\n",
    "        ymax = int(pred[1][0][3] * yScaleFactor)\n",
    "        \n",
    "        cv.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 0, 255), 2)\n",
    "        \n",
    "        output.write(frame)\n",
    "        if(cv.waitKey(10)) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    output.release()\n",
    "    cv.destroyAllWindows()\n",
    "    \n",
    "    print(\"You can now view the real-time detection video in the Ouput Video Folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d06a4b0",
   "metadata": {},
   "source": [
    "## INTERACTIVE MENU TO USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d6bd036f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Forest Fire Visualizer!\n",
      "\n",
      "Menu\n",
      "0. Quit\n",
      "1. Get Statistics about a specific California County\n",
      "2. View Forest Fire Area Predictor Model\n",
      "3. View Smoke Detection Model \n",
      "\n",
      "Please enter the number beside the menu selection that you would like to view: 1\n",
      "Please enter the name of the county: Monterey\n",
      "\n",
      "Statistics for Monterey:\n",
      "Total Incidents: 45\n",
      "Total Acres Burned: 156566.0\n",
      "Average Acres Burned per Incident: 3479.24\n",
      "Welcome to the Forest Fire Visualizer!\n",
      "\n",
      "Menu\n",
      "0. Quit\n",
      "1. Get Statistics about a specific California County\n",
      "2. View Forest Fire Area Predictor Model\n",
      "3. View Smoke Detection Model \n",
      "\n",
      "Please enter the number beside the menu selection that you would like to view: 0\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    user_input = displayBasicMenu()\n",
    "    \n",
    "    next_input = -1\n",
    "    \n",
    "    if user_input == 0:\n",
    "        break\n",
    "    elif user_input == 1:\n",
    "        next_input = input(\"Please enter the name of the county: \")\n",
    "        \n",
    "        stats = get_statistics_county(next_input)\n",
    "        print(stats)\n",
    "        \n",
    "    elif user_input == 2:\n",
    "        next_input = displayAreaPredictorMenu()\n",
    "        \n",
    "        if next_input == 0:\n",
    "            break\n",
    "        elif next_input == 1:\n",
    "            frpPredictorInformation()\n",
    "        elif next_input == 2:\n",
    "            viewHeatMap()\n",
    "            break\n",
    "        elif next_input == 3:\n",
    "            predictFRP()\n",
    "        \n",
    "    elif user_input == 3:\n",
    "        next_input = displaySmokeDetectionMenu()\n",
    "        \n",
    "        if next_input == 0:\n",
    "            break\n",
    "        elif next_input == 1:\n",
    "            smokeDetectionInformation()\n",
    "        elif next_input == 2:\n",
    "            predictOnFile()\n",
    "            break\n",
    "        elif next_input == 3:\n",
    "            predictOnRandom()\n",
    "            break\n",
    "        elif next_input == 4:\n",
    "            getIoUAccuracy()\n",
    "        elif next_input == 5:\n",
    "            applyToVideo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce60454",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
